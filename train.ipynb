{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bcaed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137768eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda import get_device_name\n",
    "\n",
    "name = get_device_name()\n",
    "\n",
    "print(name)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ade6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "# Transform: convert the dataset + normalize to [-1,1]\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5),)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37006dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af81fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = ConcatDataset([train_dataset, test_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a06aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "dataloader = DataLoader(full_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Total images: {len(full_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8507c491",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Total training images : {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e741cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get one batch of images and labels\n",
    "images, labels = next(iter(dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b95d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images * 0.5 + 0.5  # Since we normalized earlier with mean=0.5, std=0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cc1e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot first 25 images\n",
    "plt.figure(figsize=(6, 6))\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.imshow(images[i][0], cmap='gray')  # [0] to drop the channel dim (1, 28, 28) → (28, 28)\n",
    "    plt.title(f'{labels[i].item()}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392b67b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Number of steps in the diffusion process\n",
    "T = 1000\n",
    "\n",
    "# Linear noise schedule from 1e-4 to 0.02\n",
    "beta_start = 1e-4\n",
    "beta_end = 0.02\n",
    "betas = torch.linspace(beta_start, beta_end, T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5017ea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precalculate alphas\n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, dim=0)   # product of alphas over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7c6f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are useful for the closed-form noise sampling\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod).to(device)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - alphas_cumprod).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52673df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_diffusion_sample(x_0, t, device=device):\n",
    "    \"\"\"\n",
    "    Takes an image and a timestampt t\n",
    "    returns the noised version x_t and the noise used\n",
    "    \"\"\"\n",
    "    # Ensure x_0 is on the correct device\n",
    "    # x_0 = x_0.to(device)\n",
    "    \n",
    "    # # Ensure t is on the correct device\n",
    "    # t = t.to(device)\n",
    "    \n",
    "    # Generate noise on the correct device\n",
    "    noise = torch.randn_like(x_0).to(device)\n",
    "    \n",
    "    # Get the noise schedule values and ensure they're on the correct device\n",
    "    sqrt_alphas_cumprod_t = sqrt_alphas_cumprod[t][:, None, None, None].to(device)  # Fixed the indexing\n",
    "    sqrt_one_minus_alphas_cumprod_t = sqrt_one_minus_alphas_cumprod[t][:, None, None, None].to(device)  # Fixed the indexing\n",
    "\n",
    "    return sqrt_alphas_cumprod_t * x_0 + sqrt_one_minus_alphas_cumprod_t * noise, noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da038ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picking the first image from the dataset\n",
    "image = images[0].unsqueeze(0).to(device)  \n",
    "\n",
    "# picking some random timesteps to visualize how the diffusion process looks like\n",
    "timesteps = torch.tensor([0, 200, 500, 999]).to(device)\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "for idx, t in enumerate(timesteps):\n",
    "    t_batch = torch.tensor([t])\n",
    "    noisy_image, _ = forward_diffusion_sample(image, t_batch)\n",
    "    noisy_image = noisy_image.squeeze().detach().cpu().numpy()\n",
    "    \n",
    "    plt.subplot(1, 4, idx + 1)\n",
    "    plt.imshow(noisy_image, cmap='gray')\n",
    "    plt.title(f\"t = {t.item()}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20475c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "\n",
    "# Positional embedding which basically tells the UNet model 'at which position we at'\n",
    "# It is used for the model to understand what a timestep exactly means and understand how close we are at from pure noise\n",
    "# Nearby 't' have some sort of relation, that the x(t-1) is the previous step to x(t)\n",
    "# If t=999, we are very far from diffusion and if t=10, we are close to recreating the original image\n",
    "class SinusodialTimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, t):\n",
    "        # t: (batch,)\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
    "        emb = t[:,None] * emb[None,:]   # shape (batch, half_dim)\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180eed7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "        self.shortcut = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        x += self.shortcut(identity)\n",
    "        return F.relu(x)\n",
    "\n",
    "class EnhancedUNet(nn.Module):\n",
    "    def __init__(self, time_dim=256):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusodialTimeEmbedding(time_dim),\n",
    "            nn.Linear(time_dim, time_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(time_dim, time_dim)\n",
    "        )\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1 = ResidualBlock(1, 64)\n",
    "        self.enc2 = ResidualBlock(64, 128)\n",
    "        self.enc3 = ResidualBlock(128, 256)\n",
    "\n",
    "        # Time projections\n",
    "        self.time_proj1 = nn.Linear(time_dim, 64)\n",
    "        self.time_proj2 = nn.Linear(time_dim, 128)\n",
    "        self.time_proj3 = nn.Linear(time_dim, 256)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = ResidualBlock(256, 256)\n",
    "\n",
    "        # Decoder\n",
    "        self.dec1 = ResidualBlock(512, 128)  # 256 + 256 from skip\n",
    "        self.dec2 = ResidualBlock(256, 64)   # 128 + 128 from skip\n",
    "        self.dec3 = nn.Conv2d(128, 1, 3, padding=1)  # 64 + 64 from skip\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_emb = self.time_mlp(t)\n",
    "\n",
    "        # Encoder\n",
    "        x1 = self.enc1(x) + self.time_proj1(t_emb).view(-1, 64, 1, 1)\n",
    "        x2 = self.enc2(x1) + self.time_proj2(t_emb).view(-1, 128, 1, 1)\n",
    "        x3 = self.enc3(x2) + self.time_proj3(t_emb).view(-1, 256, 1, 1)\n",
    "\n",
    "        # Bottleneck\n",
    "        x = self.bottleneck(x3)\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        x = self.dec1(torch.cat([x, x3], dim=1))\n",
    "        x = self.dec2(torch.cat([x, x2], dim=1))\n",
    "        x = self.dec3(torch.cat([x, x1], dim=1))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972307be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EnhancedUNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "mse = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3e096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = 0\n",
    "for name, parameter in model.named_parameters():\n",
    "    if not parameter.requires_grad: continue\n",
    "    params = parameter.numel()\n",
    "    total_params+=params\n",
    "\n",
    "print(f\"Total Trainable Params: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87024cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSNR function to evaluate the model performance\n",
    "# this helps us check whether the generated image actually makes sense or is some random noise\n",
    "# UPDATE: THIS IS NOT THE BEST WAY TO EVALUATE THE MODEL PERFORMANCE\n",
    "def psnr(pred, target, max_val=1.0):\n",
    "    mse = F.mse_loss(pred, target, reduction='mean')\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    return 20 * torch.log10(max_val / torch.sqrt(mse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63a054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics.image import FrechetInceptionDistance\n",
    "\n",
    "# A better way to evaluate diffusion models is to use FID scores\n",
    "# They essentially measure the distance between two signals and is able to evaluate how much an image makes sense\n",
    "# for a few images, human evaluation would be good but for a larger dataset FID is the most optimal choice\n",
    "fid = FrechetInceptionDistance(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99c1be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics.image import FrechetInceptionDistance\n",
    "import torch\n",
    "import tqdm\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# A better way to evaluate diffusion models is to use FID scores\n",
    "# They measure the distance between real and generated image distributions\n",
    "fid = FrechetInceptionDistance(device=device)\n",
    "\n",
    "epochs = 200\n",
    "start_epoch = 0  # Will be updated if loading from checkpoint\n",
    "\n",
    "# Load checkpoint if it exists\n",
    "checkpoint_path = \"checkpoint_fid_guided.pth\"\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    losses_per_epoch = checkpoint['losses_per_epoch']\n",
    "    fid_per_epoch = checkpoint['fid_per_epoch']\n",
    "    print(f\"Resumed training from epoch {start_epoch}\")\n",
    "else:\n",
    "    losses_per_epoch = []\n",
    "    fid_per_epoch = []\n",
    "\n",
    "print(\"Training setup complete. Ready to start...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a2e49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD VALUE\n",
    "\n",
    "import torch\n",
    "import tqdm\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "epochs = 200\n",
    "start_epoch = 0  # Will be updated if loading from checkpoint\n",
    "\n",
    "# Load checkpoint if it exists\n",
    "checkpoint_path = \"checkpoint_fid_guided.pth\"\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    losses_per_epoch = checkpoint['losses_per_epoch']\n",
    "    # psnrs_per_epoch = checkpoint['psnrs_per_epoch']\n",
    "    fid_per_epoch = checkpoint['fid_per_epoch']\n",
    "    print(f\"Resumed training from epoch {start_epoch}\")\n",
    "else:\n",
    "    losses_per_epoch = []\n",
    "    # psnrs_per_epoch = []\n",
    "    fid_per_epoch = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "    pbar = tqdm.tqdm(dataloader)\n",
    "\n",
    "    total_loss = 0\n",
    "    step_count = 0\n",
    "    # total_psnr = 0\n",
    "    total_fid = 0\n",
    "\n",
    "    for step, (x, _) in enumerate(pbar):\n",
    "        # Ensure the input has 3 channels as expected by the model\n",
    "        x = x.to('cuda:0')\n",
    "        if x.shape[1] != 1:  # Ensure the input has 1 channel as expected by the model\n",
    "            raise RuntimeError(f\"Expected input to have 1 channel, but got {x.shape[1]} channels instead.\")\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        t = torch.randint(0, T, (batch_size,), device='cuda:0').long()\n",
    "        x_t, noise = forward_diffusion_sample(x, t)\n",
    "        noise_pred = model(x_t, t)\n",
    "\n",
    "        loss = mse(noise_pred, noise)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        alpha_bar = sqrt_alphas_cumprod[t][:, None, None, None]\n",
    "        one_minus_alpha_bar = sqrt_one_minus_alphas_cumprod[t][:, None, None, None]\n",
    "        x0_pred = (x_t - one_minus_alpha_bar * noise_pred) / alpha_bar\n",
    "        x0_pred = torch.clamp(x0_pred, -1, 1)\n",
    "\n",
    "        # batch_psnr = psnr(x0_pred, x)\n",
    "        fid.update(x0_pred, x)\n",
    "\n",
    "        fid_score = fid.compute()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        # total_psnr += batch_psnr.item()\n",
    "        total_fid += fid_score\n",
    "        step_count += 1\n",
    "\n",
    "        # pbar.set_description(f\"Loss: {loss.item():.4f} | PSNR: {batch_psnr.item():.2f}\")\n",
    "        pbar.set_description(f\"Loss: {loss.item():0.4f} | FID: {fid_score:0.2f}\")\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    avg_loss = total_loss / step_count\n",
    "    # avg_psnr = total_psnr / step_count\n",
    "    avg_fid = total_fid / step_count\n",
    "\n",
    "    losses_per_epoch.append(avg_loss)\n",
    "    # psnrs_per_epoch.append(avg_psnr)\n",
    "    fid_per_epoch.append(avg_fid)\n",
    "\n",
    "    # print(f\"Average Loss: {avg_loss:.6f} | Average PSNR: {avg_psnr:.2f}\")\n",
    "    print(f\"Average Loss: {avg_loss:.6f} | Average FID: {avg_fid:.2f}\")\n",
    "\n",
    "    # Save checkpoint after each epoch\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'losses_per_epoch': losses_per_epoch,\n",
    "        # 'psnrs_per_epoch': psnrs_per_epoch,\n",
    "        'fid_per_epoch': fid_per_epoch\n",
    "    }, checkpoint_path)\n",
    "    print(f\"Checkpoint saved at epoch {epoch+1}\")\n",
    "\n",
    "    # Plot every epoch\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, epoch + 2), losses_per_epoch, label='MSE Loss')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss vs Epoch\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # plt.subplot(1, 2, 2)\n",
    "    # plt.plot(range(1, epoch + 2), psnrs_per_epoch, label='PSNR (dB)', color='orange')\n",
    "    # plt.xlabel(\"Epoch\")\n",
    "    # plt.ylabel(\"PSNR\")\n",
    "    # plt.title(\"PSNR vs Epoch\")\n",
    "    # plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, epoch + 2), fid_score, label='FID', color='green')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"FID\")\n",
    "    plt.title(\"FID vs Epoch\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fid.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb594826",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "    pbar = tqdm.tqdm(dataloader)\n",
    "\n",
    "    total_loss = 0\n",
    "    step_count = 0\n",
    "\n",
    "    model.train()\n",
    "    for step, (x, _) in enumerate(pbar):\n",
    "        x = x.to(device)  # Move to GPU\n",
    "        if x.shape[1] != 1:  # Ensure 1 channel for MNIST\n",
    "            raise RuntimeError(f\"Expected input to have 1 channel, but got {x.shape[1]} channels.\")\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        t = torch.randint(0, T, (batch_size,), device=device).long()\n",
    "        x_t, noise = forward_diffusion_sample(x, t)\n",
    "        noise_pred = model(x_t, t.float())  # Assuming no labels for simplicity\n",
    "\n",
    "        loss = mse(noise_pred, noise)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        alpha_bar = sqrt_alphas_cumprod[t][:, None, None, None]\n",
    "        one_minus_alpha_bar = sqrt_one_minus_alphas_cumprod[t][:, None, None, None]\n",
    "        x0_pred = (x_t - one_minus_alpha_bar * noise_pred) / alpha_bar\n",
    "        x0_pred = torch.clamp(x0_pred, -1, 1)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        step_count += 1\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    avg_loss = total_loss / step_count\n",
    "    losses_per_epoch.append(avg_loss)\n",
    "\n",
    "    # Compute FID every 10 epochs\n",
    "    if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "        print(\"Computing FID score...\")\n",
    "        model.eval()\n",
    "        fid.reset()\n",
    "        # Generate samples for FID\n",
    "        n_samples = 1000  # Reasonable sample size\n",
    "        labels = torch.randint(0, 10, (n_samples,)).to(device)\n",
    "        labels_one_hot = F.one_hot(labels, num_classes=10).float()\n",
    "        x_gen = torch.randn((n_samples, 1, 28, 28)).to(device)\n",
    "        for t in reversed(range(1, T)):\n",
    "            t_batch = torch.full((n_samples,), t, device=device, dtype=torch.long)\n",
    "            beta_t = betas[t].to(device)\n",
    "            sqrt_one_minus_alphas_cumprod_t = sqrt_one_minus_alphas_cumprod[t].to(device)\n",
    "            sqrt_recip_alpha_t = (1.0 / torch.sqrt(alphas[t])).to(device)\n",
    "            epsilon_theta = model(x_gen, t_batch.float(), labels_one_hot)\n",
    "            model_mean = sqrt_recip_alpha_t * (x_gen - beta_t * epsilon_theta / sqrt_one_minus_alphas_cumprod_t)\n",
    "            if t > 1:\n",
    "                noise = torch.randn_like(x_gen).to(device)\n",
    "                sigma_t = torch.sqrt(beta_t)\n",
    "                x_gen = model_mean + sigma_t * noise\n",
    "            else:\n",
    "                x_gen = model_mean\n",
    "        x_gen = torch.clamp(x_gen, -1, 1) * 0.5 + 0.5  # Unnormalize\n",
    "\n",
    "        # Update FID with real data (first batch of dataloader as real reference)\n",
    "        real_data = next(iter(dataloader))[0].to(device)  # Shape: (batch_size, 1, 28, 28)\n",
    "        real_data = real_data.expand(-1, 3, -1, -1)  # Simulate 3 channels for Inception\n",
    "        x_gen = x_gen.expand(-1, 3, -1, -1)  # Simulate 3 channels\n",
    "        fid.update(real_data[:n_samples//len(dataloader)], real_data=True)\n",
    "        fid.update(x_gen, real_data=False)\n",
    "        fid_score = fid.compute()\n",
    "        fid_per_epoch.append(fid_score.item())\n",
    "        print(f\"FID score at epoch {epoch+1}: {fid_score.item():.2f}\")\n",
    "        model.train()\n",
    "\n",
    "    print(f\"Average Loss: {avg_loss:.6f}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'losses_per_epoch': losses_per_epoch,\n",
    "        'fid_per_epoch': fid_per_epoch\n",
    "    }, checkpoint_path)\n",
    "    print(f\"Checkpoint saved at epoch {epoch+1}\")\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, epoch + 2), losses_per_epoch, label='MSE Loss')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss vs Epoch\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, len(fid_per_epoch) + 1), fid_per_epoch, label='FID', color='green')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"FID\")\n",
    "    plt.title(\"FID vs Epoch\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30f1bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(model, n_samples, img_size=28, channels=1):\n",
    "    model.eval()\n",
    "    x = torch.randn((n_samples, channels, img_size, img_size)).to(device)\n",
    "\n",
    "    for t in reversed(range(1, T)):\n",
    "        t_batch = torch.full((n_samples,), t, device=device, dtype=torch.long)\n",
    "\n",
    "        beta_t = betas[t].to(device)\n",
    "        sqrt_one_minus_alphas_cumprod_t = sqrt_one_minus_alphas_cumprod[t].to(device)\n",
    "        sqrt_recip_alpha_t = (1.0 / torch.sqrt(alphas[t])).to(device)\n",
    "\n",
    "        # Predict the noise\n",
    "        epsilon_theta = model(x, t_batch)\n",
    "\n",
    "        # Predict x_0\n",
    "        model_mean = sqrt_recip_alpha_t * (x - beta_t / sqrt_one_minus_alphas_cumprod_t * epsilon_theta)\n",
    "\n",
    "        # No noise added if t==1 (final step)\n",
    "        if t > 1:\n",
    "            noise = torch.randn_like(x).to(device)\n",
    "            sigma_t = torch.sqrt(beta_t)\n",
    "            x = model_mean + sigma_t * noise\n",
    "        else:\n",
    "            x = model_mean\n",
    "\n",
    "    x = torch.clamp(x, -1, 1)  # Ensure pixel values are in [-1, 1]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07c6642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the generated images\n",
    "samples = sample(model, n_samples=25)\n",
    "\n",
    "# Unnormalize the images\n",
    "samples = samples * 0.5 + 0.5  # Since we normalized earlier\n",
    "\n",
    "# Plot the generated samples\n",
    "plt.figure(figsize=(6, 6))\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.imshow(samples[i][0].detach().cpu().numpy(), cmap='gray')  # [0] to drop the channel dim (1, 28, 28) → (28, 28)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
